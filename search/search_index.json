{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"mr-seek \ud83d\udd2c Mendelian randomization pipeline This is the home of the pipeline, mr-seek. Its long-term goals: to perform Mendelian randomization analysis like no pipeline before! Overview \u00b6 Welcome to mr-seek's documentation! This guide is the main source of documentation for users that are getting started with the Mendelian randomization pipeline . The ./mr-seek pipeline is composed several inter-related sub commands to setup and run the pipeline across different systems. Each of the available sub commands perform different functions: mr-seek run : Run the mr-seek pipeline with your input files. mr-seek unlock : Unlocks a previous runs output directory. mr-seek cache : Cache remote resources locally, coming soon! mr-seek is a comprehensive mendelian randomization pipeline. It relies on technologies like Singularity 1 to maintain the highest-level of reproducibility. The pipeline consists of a series of data processing and quality-control steps orchestrated by Snakemake 2 , a flexible and scalable workflow management system, to submit jobs to a cluster. The pipeline is compatible with data generated from Illumina short-read sequencing technologies. As input, it accepts a set of QTL files and outcome phenotypes and can be run locally on a compute instance or on-premise using a cluster. A user can define the method or mode of execution. The pipeline can submit jobs to a cluster using a job scheduler like SLURM (more coming soon!). A hybrid approach ensures the pipeline is accessible to all users. Before getting started, we highly recommend reading through the usage section of each available sub command. For more information about issues or trouble-shooting a problem, please checkout our FAQ prior to opening an issue on Github . Contribute \u00b6 This site is a living document, created for and by members like you. mr-seek is maintained by the members of NCBR and is improved by continuous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our GitHub repository . Citation \u00b6 If you use this software, please cite it as below: BibTex @software{Chen_Kuhn_OpenOmics_mr-seek_2025, author = {Chen, Vicky and Kuhn, Skyler and Paul, Subrata and Redekar, Neelam}, title = {OpenOmics/mr-seek}, month = mar, year = 2025, publisher = {Zenodo}, doi = {10.5281/zenodo.15096585}, url = {https://doi.org/10.5281/zenodo.15096585} } APA Chen, V., Kuhn, S., Paul, S., & Redekar, N. (2025). OpenOmics/mr-seek. Zenodo. https://doi.org/10.5281/zenodo.15096585 For more citation style options, please visit the pipeline's Zenodo page . References \u00b6 1. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. 2. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600.","title":"About"},{"location":"#overview","text":"Welcome to mr-seek's documentation! This guide is the main source of documentation for users that are getting started with the Mendelian randomization pipeline . The ./mr-seek pipeline is composed several inter-related sub commands to setup and run the pipeline across different systems. Each of the available sub commands perform different functions: mr-seek run : Run the mr-seek pipeline with your input files. mr-seek unlock : Unlocks a previous runs output directory. mr-seek cache : Cache remote resources locally, coming soon! mr-seek is a comprehensive mendelian randomization pipeline. It relies on technologies like Singularity 1 to maintain the highest-level of reproducibility. The pipeline consists of a series of data processing and quality-control steps orchestrated by Snakemake 2 , a flexible and scalable workflow management system, to submit jobs to a cluster. The pipeline is compatible with data generated from Illumina short-read sequencing technologies. As input, it accepts a set of QTL files and outcome phenotypes and can be run locally on a compute instance or on-premise using a cluster. A user can define the method or mode of execution. The pipeline can submit jobs to a cluster using a job scheduler like SLURM (more coming soon!). A hybrid approach ensures the pipeline is accessible to all users. Before getting started, we highly recommend reading through the usage section of each available sub command. For more information about issues or trouble-shooting a problem, please checkout our FAQ prior to opening an issue on Github .","title":"Overview"},{"location":"#contribute","text":"This site is a living document, created for and by members like you. mr-seek is maintained by the members of NCBR and is improved by continuous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our GitHub repository .","title":"Contribute"},{"location":"#citation","text":"If you use this software, please cite it as below: BibTex @software{Chen_Kuhn_OpenOmics_mr-seek_2025, author = {Chen, Vicky and Kuhn, Skyler and Paul, Subrata and Redekar, Neelam}, title = {OpenOmics/mr-seek}, month = mar, year = 2025, publisher = {Zenodo}, doi = {10.5281/zenodo.15096585}, url = {https://doi.org/10.5281/zenodo.15096585} } APA Chen, V., Kuhn, S., Paul, S., & Redekar, N. (2025). OpenOmics/mr-seek. Zenodo. https://doi.org/10.5281/zenodo.15096585 For more citation style options, please visit the pipeline's Zenodo page .","title":"Citation"},{"location":"#references","text":"1. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. 2. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600.","title":"References"},{"location":"license/","text":"MIT License \u00b6 Copyright \u00a9 2022 OpenOmics Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright \u00a9 2022 OpenOmics Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"faq/questions/","text":"Frequently Asked Questions \u00b6 This page is still under construction. If you need immediate help, please open an issue on Github!","title":"General Questions"},{"location":"faq/questions/#frequently-asked-questions","text":"This page is still under construction. If you need immediate help, please open an issue on Github!","title":"Frequently Asked Questions"},{"location":"usage/cache/","text":"mr-seek cache \u00b6 1. About \u00b6 The mr-seek executable is composed of several inter-related sub commands. Please see mr-seek -h for all available options. This part of the documentation describes options and concepts for mr-seek cache sub command in more detail. With minimal configuration, the cache sub command enables you to cache remote resources for the mr-seek pipeline. Caching remote resources allows the pipeline to run in an offline mode. The cache sub command can also be used to pull our pre-built reference bundles onto a new cluster or target system. The cache sub command creates local cache on the filesysytem for resources hosted on DockerHub or AWS S3. These resources are normally pulled onto the filesystem when the pipeline runs; however, due to network issues or DockerHub pull rate limits, it may make sense to pull the resources once so a shared cache can be created and re-used. It is worth noting that a singularity cache cannot normally be shared across users. Singularity strictly enforces that its cache is owned by the user. To get around this issue, the cache subcommand can be used to create local SIFs on the filesystem from images on DockerHub. 2. Synopsis \u00b6 Coming Soon!","title":"mr-seek cache"},{"location":"usage/cache/#mr-seek-cache","text":"","title":"mr-seek cache"},{"location":"usage/cache/#1-about","text":"The mr-seek executable is composed of several inter-related sub commands. Please see mr-seek -h for all available options. This part of the documentation describes options and concepts for mr-seek cache sub command in more detail. With minimal configuration, the cache sub command enables you to cache remote resources for the mr-seek pipeline. Caching remote resources allows the pipeline to run in an offline mode. The cache sub command can also be used to pull our pre-built reference bundles onto a new cluster or target system. The cache sub command creates local cache on the filesysytem for resources hosted on DockerHub or AWS S3. These resources are normally pulled onto the filesystem when the pipeline runs; however, due to network issues or DockerHub pull rate limits, it may make sense to pull the resources once so a shared cache can be created and re-used. It is worth noting that a singularity cache cannot normally be shared across users. Singularity strictly enforces that its cache is owned by the user. To get around this issue, the cache subcommand can be used to create local SIFs on the filesystem from images on DockerHub.","title":"1. About"},{"location":"usage/cache/#2-synopsis","text":"Coming Soon!","title":"2. Synopsis"},{"location":"usage/run/","text":"mr-seek run \u00b6 1. About \u00b6 The mr-seek executable is composed of several inter-related sub commands. Please see mr-seek -h for all available options. This part of the documentation describes options and concepts for mr-seek run sub command in more detail. With minimal configuration, the run sub command enables you to start running mr-seek pipeline. Setting up the mr-seek pipeline is fast and easy! In its most basic form, mr-seek run only has three required inputs . 2. Synopsis \u00b6 $ mr-seek run [--help] \\ [--dry-run] [--job-name JOB_NAME] [--mode {slurm,local}] \\ [--sif-cache SIF_CACHE] [--singularity-cache SINGULARITY_CACHE] \\ [--silent] [--threads THREADS] [--tmp-dir TMP_DIR] \\ [--input_qtl INPUT_QTL] [--pop POP] [--keyword] \\ [--outcome_pval_threshold PVAL_THRESHOLD] [--clump] \\ --exposure EXPOSURE \\ --outcome OUTCOME \\ --output OUTPUT \\ --database DATABASE The synopsis for each command shows its arguments and their usage. Optional arguments are shown in square brackets. A user must provide a list of exposure and outcome to analyze via --exposure and --outcome arguments, a database via --database argument where the outcome phenotypes will be extracted, and an output directory to store results via --output argument. Use you can always use the -h option for information on a specific command. 2.1 Required arguments \u00b6 Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --exposure EXPOSURE [EXPOSURE ...] Input exposure QTL or IEU phenotype list. type: file The file should be either a list of exposures available in a database or exposure file to read and process. If a QTL file is provided then an additional flag should be used to define the format it is. If multiple files are provided from the command-line, each input file should separated by a space. Globbing is supported! The exposure in template1 format is expecting the columns SNP, EAF, Fx, T, and log10P in the file. Additional columns may be included, if the column names do not overlap with the expected columns. Here is an example file in template1 format SNP,EAF,RSq,Fx,T,log10P chr11:74892136:G:A:rs4944963,0.25609756097561,4.71878026147098,121.891586792572,-641.982748296929 chr11:74931506:G:A:rs10899051,0.25609756097561,4.71878026147098,121.891586792572,-641.982748296929 chr11:74933260:C:T:rs7102619,0.25609756097561,4.71878026147098,121.891586792572,-641.982748296929 chr11:74935168:T:C:rs10899052,0.25609756097561,4.71878026147098,121.891586792572,-641.982748296929 Where SNP: Comma separated containing information about the SNP in the following order: SNP chromosome number SNP chromosome position Reference allele Alternate allele RSID EAF: Effect allele frequency Fx: Effect size T: T-statistic (also obtained by effect size / standard error) log10P: P-value in log10P form The exposure in pqtl format is expecting the columns SNP, rsID, BETA, SE, A1FREQ, log10 in the file, with SNP being the first column in the file. Additional columns may be included, if the column names do not overlap with the expected columns. Here is an example file in pqtl format Variant ID (CHROM:GENPOS (hg37):A0:A1),rsID,A1FREQ_discovery,BETA_discovery,SE_discovery,log10p_discovery 17:41140545:C:T,rs323500,0.2903,0.133,0.008,63.6 3:52004097:C:CG,rs373373105,0.0121,-1.444,0.035,368.1 3:38170810:C:G,rs156265,0.1441,-0.126,0.011,31.3 15:89252012:T:A,rs11073804,0.1981,0.133,0.009,52.8 Where SNP: First column in the file. Comma separated containing information about the SNP in the following order: SNP chromosome number SNP chromosome position Reference allele Alternate allele rsID: dbSNP Reference SNP number (unique SNP identifier) BETA: Effect size SE: Standard error A1FREQ: Effect allele frequency log10P: P-value in -log10P form Example: --exposure pQTL.csv --outcome OUTCOME [OUTCOME ...] Input phenotype or keyword list. type: file The file should be a list of outcomes available in a database or keywords to extract outcomes based on, with one entry per line. Currently support is only available for one file at a time. An additional flag should be used to specify if the input is a list of query keywords. Example: --outcome outcome.csv --output OUTPUT Path to an output directory. type: path This location is where the pipeline will create all of its output files, also known as the pipeline's working directory. If the provided output directory does not exist, it will be created automatically. Example: --output /data/$USER/mr-seek_out --database {ieu, neale} Database to use type: string Database to extract phenotypes from when a list of phenotypes are provided. The ieu option would extract data for the IEU GWAS database made available through the R package ieugwasr . The neale option would use the PAN-UK Biobank data hosted on Biowulf. The PAN-UK Biobank data will be processed prior to analysis. Example: --database neale 2.2 Analysis options \u00b6 Each of the following arguments are optional, and do not need to be provided. --input_qtl {pqtl, template1} Type of QTL file provided type: string When exposure file is a quantitative trait locus file to process, this flag should be used to define the type of QTL file it is. Valid options are pqtl or template1. Example: --input_qtl pqtl --pop {AFR, AMR, EAS, EUR} Super-population to use type: string Super-population to use when extracting data from Neale (PAN-UK BioBank) or when clumping data. Currently only populations shared with the 1000 Genomes project is supported. Valid options for this are: AFR, AMR, EAS, EUR. If this option is not provided it will default to EUR. Example: --pop EUR --outcome_pval_threshold THRESHOLD P-Value threshold to filter PAN-UK Biobank SNPs type: float Float value that will be used as a p-value threshold to filter the PAN-UK Biobank SNPs. If no threshold is provided then no filter would be used. Example: --outcome_pval_threshold 0.01 --clump Perform clumping on exposure data type: boolean flag Perform clumping on the data. Clumping will be run with the super-population available in the 1000 genomes reference panel. Example: --clump 2.3 Orchestration options \u00b6 Each of the following arguments are optional, and do not need to be provided. --dry-run Dry run the pipeline. type: boolean flag Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run --silent Silence standard output. type: boolean flag Reduces the amount of information directed to standard output when submitting master job to the job scheduler. Only the job id of the master job is returned. Example: --silent --mode {slurm,local} Execution Method. type: string default: slurm Execution Method. Defines the mode or method of execution. Vaild mode options include: slurm or local. slurm The slurm execution method will submit jobs to the SLURM workload manager . It is recommended running mr-seek in this mode as execution will be significantly faster in a distributed environment. This is the default mode of execution. local Local executions will run serially on compute instance. This is useful for testing, debugging, or when a user does not have access to a high performance computing environment. If this option is not provided, it will default to a local execution mode. Example: --mode slurm --job-name JOB_NAME Set the name of the pipeline's master job. type: string default: pl:mr-seek When submitting the pipeline to a job scheduler, like SLURM, this option always you to set the name of the pipeline's master job. By default, the name of the pipeline's master job is set to \"pl:mr-seek\". Example: --job-name pl_id-42 --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The mr-seek cache subcommand can be used to create a local SIF cache. Please see mr-seek cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running mr-seek with this option when ever possible. Example: --singularity-cache /data/$USER/SIFs --threads THREADS Max number of threads for each process. type: int default: 2 Max number of threads for each process. This option is more applicable when running the pipeline with --mode local . It is recommended setting this vaule to the maximum number of CPUs available on the host machine. Example: --threads 12 --tmp-dir TMP_DIR Path to temporary directory. type: path default: /lscratch/$SLURM_JOBID Path on the file system for writing temporary output files. By default, the temporary directory is set to '/lscratch/$SLURM_JOBID' for backwards compatibility with the NIH's Biowulf cluster; however, if you are running the pipeline on another cluster, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject a variable into this string that should NOT be expanded, please quote this options value in single quotes. Example: --tmp-dir /scratch/$USER/ 2.4 Miscellaneous options \u00b6 Each of the following arguments are optional, and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help 3. Example \u00b6 # Step 1.) Grab an interactive node, # do not run on head node! srun -N 1 -n 1 --time = 1 :00:00 --mem = 8gb --cpus-per-task = 2 --pty bash module purge module load singularity snakemake # Step 2A.) Dry-run the pipeline ./mr-seek run --exposure .tests/template1.csv \\ --outcome .tests/ieu_10.csv \\ --output /data/ $USER /output \\ --input_qtl template1 \\ --database ieu \\ --mode slurm \\ --dry-run # Step 2B.) Run the mr-seek pipeline # The slurm mode will submit jobs to # the cluster. It is recommended running # the pipeline in this mode. ./mr-seek run --exposure .tests/template1.csv \\ --outcome .tests/ieu_10.csv \\ --output /data/ $USER /output \\ --input_qtl template1 \\ --database ieu \\ --mode slurm","title":"mr-seek run"},{"location":"usage/run/#mr-seek-run","text":"","title":"mr-seek run"},{"location":"usage/run/#1-about","text":"The mr-seek executable is composed of several inter-related sub commands. Please see mr-seek -h for all available options. This part of the documentation describes options and concepts for mr-seek run sub command in more detail. With minimal configuration, the run sub command enables you to start running mr-seek pipeline. Setting up the mr-seek pipeline is fast and easy! In its most basic form, mr-seek run only has three required inputs .","title":"1. About"},{"location":"usage/run/#2-synopsis","text":"$ mr-seek run [--help] \\ [--dry-run] [--job-name JOB_NAME] [--mode {slurm,local}] \\ [--sif-cache SIF_CACHE] [--singularity-cache SINGULARITY_CACHE] \\ [--silent] [--threads THREADS] [--tmp-dir TMP_DIR] \\ [--input_qtl INPUT_QTL] [--pop POP] [--keyword] \\ [--outcome_pval_threshold PVAL_THRESHOLD] [--clump] \\ --exposure EXPOSURE \\ --outcome OUTCOME \\ --output OUTPUT \\ --database DATABASE The synopsis for each command shows its arguments and their usage. Optional arguments are shown in square brackets. A user must provide a list of exposure and outcome to analyze via --exposure and --outcome arguments, a database via --database argument where the outcome phenotypes will be extracted, and an output directory to store results via --output argument. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/run/#21-required-arguments","text":"Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --exposure EXPOSURE [EXPOSURE ...] Input exposure QTL or IEU phenotype list. type: file The file should be either a list of exposures available in a database or exposure file to read and process. If a QTL file is provided then an additional flag should be used to define the format it is. If multiple files are provided from the command-line, each input file should separated by a space. Globbing is supported! The exposure in template1 format is expecting the columns SNP, EAF, Fx, T, and log10P in the file. Additional columns may be included, if the column names do not overlap with the expected columns. Here is an example file in template1 format SNP,EAF,RSq,Fx,T,log10P chr11:74892136:G:A:rs4944963,0.25609756097561,4.71878026147098,121.891586792572,-641.982748296929 chr11:74931506:G:A:rs10899051,0.25609756097561,4.71878026147098,121.891586792572,-641.982748296929 chr11:74933260:C:T:rs7102619,0.25609756097561,4.71878026147098,121.891586792572,-641.982748296929 chr11:74935168:T:C:rs10899052,0.25609756097561,4.71878026147098,121.891586792572,-641.982748296929 Where SNP: Comma separated containing information about the SNP in the following order: SNP chromosome number SNP chromosome position Reference allele Alternate allele RSID EAF: Effect allele frequency Fx: Effect size T: T-statistic (also obtained by effect size / standard error) log10P: P-value in log10P form The exposure in pqtl format is expecting the columns SNP, rsID, BETA, SE, A1FREQ, log10 in the file, with SNP being the first column in the file. Additional columns may be included, if the column names do not overlap with the expected columns. Here is an example file in pqtl format Variant ID (CHROM:GENPOS (hg37):A0:A1),rsID,A1FREQ_discovery,BETA_discovery,SE_discovery,log10p_discovery 17:41140545:C:T,rs323500,0.2903,0.133,0.008,63.6 3:52004097:C:CG,rs373373105,0.0121,-1.444,0.035,368.1 3:38170810:C:G,rs156265,0.1441,-0.126,0.011,31.3 15:89252012:T:A,rs11073804,0.1981,0.133,0.009,52.8 Where SNP: First column in the file. Comma separated containing information about the SNP in the following order: SNP chromosome number SNP chromosome position Reference allele Alternate allele rsID: dbSNP Reference SNP number (unique SNP identifier) BETA: Effect size SE: Standard error A1FREQ: Effect allele frequency log10P: P-value in -log10P form Example: --exposure pQTL.csv --outcome OUTCOME [OUTCOME ...] Input phenotype or keyword list. type: file The file should be a list of outcomes available in a database or keywords to extract outcomes based on, with one entry per line. Currently support is only available for one file at a time. An additional flag should be used to specify if the input is a list of query keywords. Example: --outcome outcome.csv --output OUTPUT Path to an output directory. type: path This location is where the pipeline will create all of its output files, also known as the pipeline's working directory. If the provided output directory does not exist, it will be created automatically. Example: --output /data/$USER/mr-seek_out --database {ieu, neale} Database to use type: string Database to extract phenotypes from when a list of phenotypes are provided. The ieu option would extract data for the IEU GWAS database made available through the R package ieugwasr . The neale option would use the PAN-UK Biobank data hosted on Biowulf. The PAN-UK Biobank data will be processed prior to analysis. Example: --database neale","title":"2.1 Required arguments"},{"location":"usage/run/#22-analysis-options","text":"Each of the following arguments are optional, and do not need to be provided. --input_qtl {pqtl, template1} Type of QTL file provided type: string When exposure file is a quantitative trait locus file to process, this flag should be used to define the type of QTL file it is. Valid options are pqtl or template1. Example: --input_qtl pqtl --pop {AFR, AMR, EAS, EUR} Super-population to use type: string Super-population to use when extracting data from Neale (PAN-UK BioBank) or when clumping data. Currently only populations shared with the 1000 Genomes project is supported. Valid options for this are: AFR, AMR, EAS, EUR. If this option is not provided it will default to EUR. Example: --pop EUR --outcome_pval_threshold THRESHOLD P-Value threshold to filter PAN-UK Biobank SNPs type: float Float value that will be used as a p-value threshold to filter the PAN-UK Biobank SNPs. If no threshold is provided then no filter would be used. Example: --outcome_pval_threshold 0.01 --clump Perform clumping on exposure data type: boolean flag Perform clumping on the data. Clumping will be run with the super-population available in the 1000 genomes reference panel. Example: --clump","title":"2.2 Analysis options"},{"location":"usage/run/#23-orchestration-options","text":"Each of the following arguments are optional, and do not need to be provided. --dry-run Dry run the pipeline. type: boolean flag Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run --silent Silence standard output. type: boolean flag Reduces the amount of information directed to standard output when submitting master job to the job scheduler. Only the job id of the master job is returned. Example: --silent --mode {slurm,local} Execution Method. type: string default: slurm Execution Method. Defines the mode or method of execution. Vaild mode options include: slurm or local. slurm The slurm execution method will submit jobs to the SLURM workload manager . It is recommended running mr-seek in this mode as execution will be significantly faster in a distributed environment. This is the default mode of execution. local Local executions will run serially on compute instance. This is useful for testing, debugging, or when a user does not have access to a high performance computing environment. If this option is not provided, it will default to a local execution mode. Example: --mode slurm --job-name JOB_NAME Set the name of the pipeline's master job. type: string default: pl:mr-seek When submitting the pipeline to a job scheduler, like SLURM, this option always you to set the name of the pipeline's master job. By default, the name of the pipeline's master job is set to \"pl:mr-seek\". Example: --job-name pl_id-42 --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The mr-seek cache subcommand can be used to create a local SIF cache. Please see mr-seek cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running mr-seek with this option when ever possible. Example: --singularity-cache /data/$USER/SIFs --threads THREADS Max number of threads for each process. type: int default: 2 Max number of threads for each process. This option is more applicable when running the pipeline with --mode local . It is recommended setting this vaule to the maximum number of CPUs available on the host machine. Example: --threads 12 --tmp-dir TMP_DIR Path to temporary directory. type: path default: /lscratch/$SLURM_JOBID Path on the file system for writing temporary output files. By default, the temporary directory is set to '/lscratch/$SLURM_JOBID' for backwards compatibility with the NIH's Biowulf cluster; however, if you are running the pipeline on another cluster, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject a variable into this string that should NOT be expanded, please quote this options value in single quotes. Example: --tmp-dir /scratch/$USER/","title":"2.3 Orchestration options"},{"location":"usage/run/#24-miscellaneous-options","text":"Each of the following arguments are optional, and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help","title":"2.4 Miscellaneous options"},{"location":"usage/run/#3-example","text":"# Step 1.) Grab an interactive node, # do not run on head node! srun -N 1 -n 1 --time = 1 :00:00 --mem = 8gb --cpus-per-task = 2 --pty bash module purge module load singularity snakemake # Step 2A.) Dry-run the pipeline ./mr-seek run --exposure .tests/template1.csv \\ --outcome .tests/ieu_10.csv \\ --output /data/ $USER /output \\ --input_qtl template1 \\ --database ieu \\ --mode slurm \\ --dry-run # Step 2B.) Run the mr-seek pipeline # The slurm mode will submit jobs to # the cluster. It is recommended running # the pipeline in this mode. ./mr-seek run --exposure .tests/template1.csv \\ --outcome .tests/ieu_10.csv \\ --output /data/ $USER /output \\ --input_qtl template1 \\ --database ieu \\ --mode slurm","title":"3. Example"},{"location":"usage/unlock/","text":"mr-seek unlock \u00b6 1. About \u00b6 The mr-seek executable is composed of several inter-related sub commands. Please see mr-seek -h for all available options. This part of the documentation describes options and concepts for mr-seek unlock sub command in more detail. With minimal configuration, the unlock sub command enables you to unlock a pipeline output directory. If the pipeline fails ungracefully, it maybe required to unlock the working directory before proceeding again. Snakemake will inform a user when it maybe necessary to unlock a working directory with an error message stating: Error: Directory cannot be locked . Please verify that the pipeline is not running before running this command. If the pipeline is currently running, the workflow manager will report the working directory is locked. The is the default behavior of snakemake, and it is normal. Do NOT run this command if the pipeline is still running! Please kill the master job and it's child jobs prior to running this command. Unlocking mr-seek pipeline output directory is fast and easy! In its most basic form, mr-seek unlock only has one required input . 2. Synopsis \u00b6 $ ./mr-seek unlock [-h] --output OUTPUT The synopsis for this command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide an output directory to unlock via --output argument. After running the unlock sub command, you can resume the build or run pipeline from where it left off by re-running it. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 --output OUTPUT Output directory to unlock. type: path Path to a previous run's output directory. This will remove a lock on the working directory. Please verify that the pipeline is not running before running this command. Example: --output /data/$USER/mr-seek_out 2.2 Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help 3. Example \u00b6 # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Unlock a pipeline output directory mr-seek unlock --output /data/ $USER /output","title":"mr-seek unlock"},{"location":"usage/unlock/#mr-seek-unlock","text":"","title":"mr-seek unlock"},{"location":"usage/unlock/#1-about","text":"The mr-seek executable is composed of several inter-related sub commands. Please see mr-seek -h for all available options. This part of the documentation describes options and concepts for mr-seek unlock sub command in more detail. With minimal configuration, the unlock sub command enables you to unlock a pipeline output directory. If the pipeline fails ungracefully, it maybe required to unlock the working directory before proceeding again. Snakemake will inform a user when it maybe necessary to unlock a working directory with an error message stating: Error: Directory cannot be locked . Please verify that the pipeline is not running before running this command. If the pipeline is currently running, the workflow manager will report the working directory is locked. The is the default behavior of snakemake, and it is normal. Do NOT run this command if the pipeline is still running! Please kill the master job and it's child jobs prior to running this command. Unlocking mr-seek pipeline output directory is fast and easy! In its most basic form, mr-seek unlock only has one required input .","title":"1. About"},{"location":"usage/unlock/#2-synopsis","text":"$ ./mr-seek unlock [-h] --output OUTPUT The synopsis for this command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide an output directory to unlock via --output argument. After running the unlock sub command, you can resume the build or run pipeline from where it left off by re-running it. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/unlock/#21-required-arguments","text":"--output OUTPUT Output directory to unlock. type: path Path to a previous run's output directory. This will remove a lock on the working directory. Please verify that the pipeline is not running before running this command. Example: --output /data/$USER/mr-seek_out","title":"2.1 Required Arguments"},{"location":"usage/unlock/#22-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help","title":"2.2 Options"},{"location":"usage/unlock/#3-example","text":"# Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Unlock a pipeline output directory mr-seek unlock --output /data/ $USER /output","title":"3. Example"}]}